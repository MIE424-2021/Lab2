{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIE424 (2021 Winter) Lab 2\n",
    "\n",
    "Yuehuan He and Jake Mosseri \n",
    "\n",
    "In this lab, you will be using linear classifier to build a spam classifier. \n",
    "\n",
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. You will use linear classifier to build your own spam filter.\n",
    "\n",
    "You will be training a classifier to classify whether a given email, $x$, is spam ($y = 1$) or non-spam ($y = 0$). In particular, you need to convert each email into a feature vector $x \\in R^n$. The following parts of the lab will walk you through how such a feature vector can be constructed from an email.\n",
    "\n",
    "Dataset used:\n",
    "\n",
    "- spamTrain.mat - Spam training set\n",
    "- spamTest.mat - Spam test set\n",
    "\n",
    "\n",
    "\n",
    "The dataset has been processed using the following feature extraction. n = # words in vocabulary list. Specifically, the feature $x_i \\in \\{0, 1\\}$ for an email corresponds to whether the $i$-th word in the dictionary occurs in the email. That is, $x_i = 1$ if the $i$-th word is in the email and $x_i$ = 0 if the $i$-th word is not present in the email.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Preprocessing process:\n",
    "\n",
    "-  Lower-casing: The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "- Stripping HTML: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "- Normalizing URLs: All URLs are replaced with the text “httpaddr”.\n",
    "- Normalizing Email Addresses: All email addresses are replaced with the text “emailaddr”.\n",
    "- Normalizing Numbers: All numbers are replaced with the text “number”.\n",
    "- Normalizing Dollars: All dollar signs are replaced with the text “dollar”.\n",
    "- Word Stemming: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "- Removal of non-words: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "\n",
    "After preprocessing the emails, we have a list of words for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out.\n",
    "\n",
    "For this example, we have chosen only the most frequently occuring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set. The complete vocabulary list is in the file vocab.txt and also shown in the figure below. Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. In practice, a vocabulary list with about 10,000 to 50,000 words is often used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoding process](img/encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. The following figure shows the mapping for the sample email. Specifically, in the sample email, the word “anyone” was first normalized to “anyon” and then mapped onto the index 86 in the vocabulary list.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoded datapoint](img/encodedx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load a preprocessed training dataset that will be used to train a linear classifier. spamTrain.mat contains training examples of spam and non-spam email, while spamTest.mat contains test examples. Each original email was processed and converted into a vector $x_{(i)} \\in R^{1899}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spam Email dataset\n",
    "train_data = loadmat('spamTrain.mat')\n",
    "test_data = loadmat('spamTest.mat')\n",
    "X_train = train_data['X']\n",
    "Y_train = train_data['y'].ravel()\n",
    "X_test  = test_data['Xtest']\n",
    "Y_test  = test_data['ytest'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary list:\n",
    "vocabulary = []\n",
    "with open('vocab.txt') as f:\n",
    "    for line in f:\n",
    "        idx, word = line.split('\\t')\n",
    "        vocabulary.append(word.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example email: \n",
    "\n",
    "*Anyone knows how much it costs to host a web portal ?*\n",
    "*Well, it depends on how many visitors youre expecting. This can be anywhere from less than 10 bucks a month to a couple of 100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..*\n",
    "*To unsubscribe yourself from this mailing list, send an email to:\n",
    "groupname-unsubscribe@egroups.com*\n",
    "\n",
    "The encoded representation of this email is given in the first figure:\n",
    "\n",
    "[86, 916, 794, 1077, ...]\n",
    "\n",
    "To check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyon'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[915]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap from Lab 1:\n",
    "\n",
    "- How to find the size of training set and test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4000 training examples and 1000 test examples\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {X_train.shape[0]} training examples and {X_test.shape[0]} test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct and Train a Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we then proceed to train a linear classifier with hinge loss to classify between spam ($y = 1$) and non-spam ($y = 0$) emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `SGDClassifier` in sklearn package to construct and train a linear classifier:\n",
    "\n",
    "class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- **loss**: str, default=’hinge’\n",
    "    - The loss function to be used. Defaults to ‘hinge’.\n",
    "    - The possible options are ‘hinge’, ‘squared_hinge’, etc.\n",
    "- **penalty**: {‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’\n",
    "    - The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "- **alpha**: float, default=0.0001\n",
    "    - Constant that multiplies the regularization term. The higher the value, the stronger the regularization. \n",
    "\n",
    "\n",
    "See more in its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(),SGDClassifier(alpha=0,learning_rate='invscaling',eta0=0.01,verbose=1))\n",
    "\n",
    "# Here alpha = 0 is used to not apply any penalty. Learning rate then cannot take the default value 'optimal'\n",
    "# and an initial stepsize needs to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.98, NNZs: 1895, Bias: -0.206604, T: 4000, Avg. loss: 0.166303\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.00, NNZs: 1895, Bias: -0.233591, T: 8000, Avg. loss: 0.073208\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.01, NNZs: 1895, Bias: -0.247319, T: 12000, Avg. loss: 0.057050\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.03, NNZs: 1895, Bias: -0.256208, T: 16000, Avg. loss: 0.047773\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 1895, Bias: -0.263470, T: 20000, Avg. loss: 0.042022\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.04, NNZs: 1895, Bias: -0.268784, T: 24000, Avg. loss: 0.037829\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.05, NNZs: 1895, Bias: -0.273459, T: 28000, Avg. loss: 0.034526\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 1895, Bias: -0.276592, T: 32000, Avg. loss: 0.031960\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 1895, Bias: -0.280032, T: 36000, Avg. loss: 0.030128\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.07, NNZs: 1895, Bias: -0.282856, T: 40000, Avg. loss: 0.028477\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.08, NNZs: 1895, Bias: -0.284701, T: 44000, Avg. loss: 0.026972\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.08, NNZs: 1895, Bias: -0.286478, T: 48000, Avg. loss: 0.025697\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.09, NNZs: 1895, Bias: -0.288752, T: 52000, Avg. loss: 0.024778\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.09, NNZs: 1895, Bias: -0.290257, T: 56000, Avg. loss: 0.023754\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.09, NNZs: 1895, Bias: -0.291550, T: 60000, Avg. loss: 0.022977\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.10, NNZs: 1895, Bias: -0.292559, T: 64000, Avg. loss: 0.022264\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.10, NNZs: 1895, Bias: -0.293498, T: 68000, Avg. loss: 0.021602\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.10, NNZs: 1895, Bias: -0.295045, T: 72000, Avg. loss: 0.021056\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.11, NNZs: 1895, Bias: -0.296294, T: 76000, Avg. loss: 0.020534\n",
      "Total training time: 0.27 seconds.\n",
      "Convergence after 19 epochs took 0.27 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('sgdclassifier',\n",
       "                 SGDClassifier(alpha=0, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.01,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='invscaling', loss='hinge',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=1, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.35000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "prediction_train = clf.predict(X_train)\n",
    "print ('Train Accuracy:', np.mean(prediction_train == Y_train) * 100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.8 %\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "prediction_test = clf.predict(X_test)\n",
    "print ('Test Accuracy:', np.mean(prediction_test == Y_test) * 100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand how the spam classifier works, we can inspect the parameters to see which words the classifier thinks are the most predictive of spam. The next step of this lab finds the parameters with the largest positive values in the classifier and displays the corresponding words. Thus, if an email contains words such as “click”, “remove”, “gurantee” etc., it is likely to be classified as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictors of spam:\n",
      "click 0.1794555324960575\n",
      "remov 0.15772664712361162\n",
      "our 0.12176842175777475\n",
      "telephon 0.11815650661818612\n",
      "here 0.11501816402040703\n",
      "below 0.11276389032880309\n",
      "nbsp 0.10881947020000925\n",
      "guarante 0.10551974303122025\n",
      "price 0.09104807991520399\n",
      "pleas 0.08813961465097749\n",
      "estat 0.08647498294887866\n",
      "fill 0.0856762101796141\n",
      "basenumb 0.08271818938311372\n",
      "absolut 0.07893865901230397\n",
      "receiv 0.07880284147332711\n"
     ]
    }
   ],
   "source": [
    "coef = clf.named_steps['sgdclassifier'].coef_.ravel()\n",
    "idx  = coef.argsort()[::-1].ravel()\n",
    "print ('Top predictors of spam:')\n",
    "for i in range(15):\n",
    "    print (vocabulary[idx[i]], coef[idx[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "- try train the model with other loss functions;\n",
    "- find an email of your own and see if the fitted model can classify it correctly (the following helper functions might be useful in preprocessing your email content)\n",
    "- train the model with a subset of the training data and see how the test accuracies compare, e.g. with 100% of training data vs with 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "def split(delimiters, string, maxsplit=0):\n",
    "    pattern = '|'.join(map(re.escape, delimiters))\n",
    "    return re.split(pattern, string, maxsplit)\n",
    "\n",
    "def process_email(email_contents,vocabulary):\n",
    "    \"\"\"\n",
    "    Preprocesses a the body of an email and returns a list of word indices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    email_contents : string\n",
    "        The email content.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of word indices.\n",
    "    \"\"\"\n",
    "    email_contents = email_contents.lower()\n",
    "    email_contents = re.sub('<[^<>]+>', ' ', email_contents)\n",
    "    email_contents = re.sub('[0-9]+', 'number', email_contents)\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*', 'httpaddr', email_contents)\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email_contents)\n",
    "    email_contents = re.sub('[$]+', 'dollar', email_contents)\n",
    "\n",
    "    words = split(\"\"\" @$/#.-:&*+=[]?!(){},'\">_<;%\\n\\r\"\"\", email_contents)\n",
    "    word_indices = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in words:\n",
    "        word = re.sub('[^a-zA-Z0-9]', '', word)\n",
    "        if word == '':\n",
    "            continue\n",
    "        word = stemmer.stem(word)\n",
    "        if word in vocabulary:\n",
    "            idx = vocabulary.index(word)\n",
    "            word_indices.append(idx)\n",
    "\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "\n",
    "- try train the model with other loss functions;\n",
    "    - to apply different loss function, you could change the parameter in `SGDClassifier`.\n",
    "    - for example: 'perceptron' loss (which we would be implementing from sketch in next Lab): SGDClassifier(loss = 'perceptron', alpha=0, learning_rate='invscaling', eta0=0.01, verbose=1)\n",
    "    - The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.\n",
    "    - The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find an email of your own and see if the fitted model can classify it correctly (the following helper functions might be useful in preprocessing your email content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heres what you need to know. audible   WISH LIST   BEST SELLERS   NEW RELEASES Its great to be a member!  Just listen to these perks Get One Audiobook Every Month Members get 1 credit a month — good for any title regardless of length or price. Explore our catalogue Receive a BONUS audiobook each month. Editors Extra gives you a free monthly audiobook chosen by our editors as part of your membership. Add to Your Library Exclusive Member Discounts Never pay full price!  Used up your monthly credits? Members can buy additional titles at any time for 30% off the list price, plus get access to exclusive sales and daily deals. Shop Now Audible Original Podcasts Unlimited listening to gripping and immersive documentaries, comedy, and more. 100% free for members. Download to lots of devices Your library is automatically synced across all your devices, so you can listen anytime, anywhere. See listening options Need help? From app questions to listening recommendations, our Customer Care team has got you covered. Contact Customer Care      Give them gifts for life   audible gift centre    Follow us on social   facebook   instagram   twitter         Unsubscribe | Help Centre | Account Details | Conditions of Use | Privacy Policy  Contact Customer Service for any questions, or if you need assistance.  You may adjust your communication preferences by updating your notification settings.  ©1997-2021 Audible, Inc. Audible and the Audible logo are trademarks of Audible, Inc., or its affiliates. 1 Washington Park, 16th Floor, Newark, NJ 07102. All Rights Reserved. \n"
     ]
    }
   ],
   "source": [
    "email_example = 'Heres what you need to know. audible   WISH LIST   BEST SELLERS   NEW RELEASES Its great to be a member!  Just listen to these perks Get One Audiobook Every Month Members get 1 credit a month — good for any title regardless of length or price. Explore our catalogue Receive a BONUS audiobook each month. Editors Extra gives you a free monthly audiobook chosen by our editors as part of your membership. Add to Your Library Exclusive Member Discounts Never pay full price!  Used up your monthly credits? Members can buy additional titles at any time for 30% off the list price, plus get access to exclusive sales and daily deals. Shop Now Audible Original Podcasts Unlimited listening to gripping and immersive documentaries, comedy, and more. 100% free for members. Download to lots of devices Your library is automatically synced across all your devices, so you can listen anytime, anywhere. See listening options Need help? From app questions to listening recommendations, our Customer Care team has got you covered. Contact Customer Care      Give them gifts for life   audible gift centre    Follow us on social   facebook   instagram   twitter         Unsubscribe | Help Centre | Account Details | Conditions of Use | Privacy Policy  Contact Customer Service for any questions, or if you need assistance.  You may adjust your communication preferences by updating your notification settings.  ©1997-2021 Audible, Inc. Audible and the Audible logo are trademarks of Audible, Inc., or its affiliates. 1 Washington Park, 16th Floor, Newark, NJ 07102. All Rights Reserved. '\n",
    "print(email_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[774, 1834, 1892, 1092, 1698, 915, 1858, 960, 175, 1098, 1391, 882, 731, 1698, 161, 1024, 901, 961, 1698, 1674, 707, 574, 1061, 1024, 707, 1119, 382, 1061, 723, 665, 74, 1696, 1161, 943, 1181, 1298, 597, 1190, 1372, 196, 500, 1061, 518, 601, 711, 1892, 680, 1062, 226, 1190, 518, 115, 1211, 1161, 1894, 1025, 22, 1698, 1894, 950, 585, 1024, 459, 1097, 691, 1298, 1759, 1894, 1062, 382, 1024, 237, 23, 1696, 123, 74, 1693, 665, 1119, 1162, 1665, 960, 1298, 1264, 707, 10, 1698, 585, 1444, 73, 393, 406, 1506, 1116, 1185, 1754, 961, 1698, 73, 73, 1063, 1119, 680, 665, 1024, 486, 1698, 979, 1161, 445, 1894, 950, 876, 137, 15, 52, 1894, 445, 1537, 1892, 237, 961, 88, 1467, 961, 1180, 1092, 770, 687, 92, 1348, 1698, 961, 1375, 1190, 387, 246, 1643, 742, 725, 1892, 377, 351, 387, 246, 711, 1668, 709, 665, 952, 709, 662, 1765, 1170, 1538, 1757, 770, 12, 439, 333, 1161, 1303, 1270, 351, 387, 1492, 665, 74, 1348, 1181, 809, 1892, 1092, 119, 1892, 1894, 319, 1288, 226, 1760, 1894, 1493, 1119, 1119, 825, 73, 1665, 1161, 825, 1181, 882, 35, 1119, 1813, 1151, 1119, 52, 1424, 1407]\n"
     ]
    }
   ],
   "source": [
    "email_example_encoded=process_email(email_example,vocabulary)\n",
    "print(email_example_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_example_x = np.zeros(len(vocabulary))\n",
    "email_example_x[email_example_encoded] = 1\n",
    "email_example_x=email_example_x.reshape(1,len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained model predicted: spam.\n"
     ]
    }
   ],
   "source": [
    "if clf.predict(email_example_x)==1:\n",
    "    print('The trained model predicted: spam.')\n",
    "else:\n",
    "    print('The trained model predicted: not a spam.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train the model with a subset of the training data and see how the test accuracies compare, e.g. with 100% of training data vs with 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.55000000000001 %\n",
      "Test Accuracy: 96.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_subset_ratio = 0.5\n",
    "X_train_subset, _,  Y_train_subset, _ = train_test_split(X_train, Y_train, train_size=train_subset_ratio, random_state = 424)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(),SGDClassifier(alpha=0,learning_rate='invscaling',eta0=0.01))\n",
    "clf.fit(X_train_subset, Y_train_subset)\n",
    "\n",
    "# Training accuracy\n",
    "prediction_train = clf.predict(X_train_subset)\n",
    "print ('Train Accuracy:', np.mean(prediction_train == Y_train_subset) * 100,'%')\n",
    "# Test accuracy\n",
    "prediction_test = clf.predict(X_test)\n",
    "print ('Test Accuracy:', np.mean(prediction_test == Y_test) * 100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subset Ratio:  0.1  Train Accuracy: 100.0 %,  Test Accuracy: 94.6 %\n",
      "Train Subset Ratio:  0.2  Train Accuracy: 99.625 %,  Test Accuracy: 95.6 %\n",
      "Train Subset Ratio:  0.3  Train Accuracy: 99.5 %,  Test Accuracy: 96.8 %\n",
      "Train Subset Ratio:  0.4  Train Accuracy: 99.5 %,  Test Accuracy: 96.3 %\n",
      "Train Subset Ratio:  0.5  Train Accuracy: 99.45 %,  Test Accuracy: 96.6 %\n",
      "Train Subset Ratio:  0.6  Train Accuracy: 99.54166666666666 %,  Test Accuracy: 97.1 %\n",
      "Train Subset Ratio:  0.7  Train Accuracy: 99.53571428571428 %,  Test Accuracy: 97.7 %\n",
      "Train Subset Ratio:  0.8  Train Accuracy: 99.40625 %,  Test Accuracy: 96.89999999999999 %\n",
      "Train Subset Ratio:  0.9  Train Accuracy: 99.47222222222221 %,  Test Accuracy: 97.7 %\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),SGDClassifier(alpha=0,learning_rate='invscaling',eta0=0.01))\n",
    "\n",
    "for i in range(9):\n",
    "    train_subset_ratio = (1+i)/10\n",
    "    X_train_subset, _,  Y_train_subset, _ = train_test_split(X_train, Y_train, train_size=train_subset_ratio, random_state = 424)\n",
    "    clf.fit(X_train_subset, Y_train_subset)\n",
    "    prediction_train = clf.predict(X_train_subset)\n",
    "    prediction_test = clf.predict(X_test)\n",
    "    print('Train Subset Ratio: ', train_subset_ratio,' Train Accuracy:', np.mean(prediction_train == Y_train_subset) * 100,'%, ','Test Accuracy:', np.mean(prediction_test == Y_test) * 100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements:\n",
    "\n",
    "Adopted from problem set and course materials in Stanford CS229 and [nex3z](https://github.com/nex3z) implementation in python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
